{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üè° Min-Max Normalization Workshop\n",
    "## Team Name: FIVE\n",
    "## Team Members: Fasalu Rahman Kottaparambu, Christo Pananjickal Baby\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùó Why We Normalize: The Problem with Raw Feature Scales\n",
    "\n",
    "In housing data, features like `Price` and `Lot_Size` can have values in the hundreds of thousands, while others like `Num_Bedrooms` range from 1 to 5. This creates problems when we use algorithms that depend on numeric magnitudes.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è What Goes Wrong Without Normalization\n",
    "\n",
    "---\n",
    "\n",
    "### 1. üß≠ K-Nearest Neighbors (KNN)\n",
    "\n",
    "KNN uses the **Euclidean distance** formula:\n",
    "\n",
    "$$\n",
    "d = \\sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2 + \\cdots}\n",
    "$$\n",
    "\n",
    "**Example:**\n",
    "\n",
    "- $ \\text{Price}_1 = 650{,}000, \\quad \\text{Price}_2 = 250{,}000 $\n",
    "- $ \\text{Bedrooms}_1 = 3, \\quad \\text{Bedrooms}_2 = 2 $\n",
    "\n",
    "Now compute squared differences:\n",
    "\n",
    "$$\n",
    "(\\text{Price}_1 - \\text{Price}_2)^2 = (650{,}000 - 250{,}000)^2 = (400{,}000)^2 = 1.6 \\times 10^{11}\n",
    "$$\n",
    "$$\n",
    "(\\text{Bedrooms}_1 - \\text{Bedrooms}_2)^2 = (3 - 2)^2 = 1\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è **Price dominates the distance calculation**, making smaller features like `Bedrooms` irrelevant.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. üìâ Linear Regression\n",
    "\n",
    "Linear regression estimates:\n",
    "\n",
    "$$\n",
    "y = \\beta_1 \\cdot \\text{Price} + \\beta_2 \\cdot \\text{Bedrooms} + \\beta_3 \\cdot \\text{Lot\\_Size} + \\epsilon\n",
    "$$\n",
    "\n",
    "If `Price` has very large values:\n",
    "- Gradient updates for $ \\beta_1 $ will be **much larger**\n",
    "- Gradient updates for $ \\beta_2 $ (Bedrooms) will be **very small**\n",
    "\n",
    "‚û°Ô∏è The model overfits high-magnitude features like `Price`.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. üß† Neural Networks\n",
    "\n",
    "A single neuron computes:\n",
    "\n",
    "$$\n",
    "z = w_1 \\cdot \\text{Price} + w_2 \\cdot \\text{Bedrooms} + w_3 \\cdot \\text{Lot\\_Size}\n",
    "$$\n",
    "\n",
    "If:\n",
    "\n",
    "- $ \\text{Price} = 650{,}000 $\n",
    "- $ \\text{Bedrooms} = 3 $\n",
    "- $ \\text{Lot\\_Size} = 8{,}000 $\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "z \\approx w_1 \\cdot 650{,}000 + w_2 \\cdot 3 + w_3 \\cdot 8{,}000\n",
    "$$\n",
    "\n",
    "‚û°Ô∏è Even with equal weights, `Price` contributes **most of the activation**, making it difficult for the network to learn from other features.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Solution: Min-Max Normalization\n",
    "\n",
    "We apply the transformation:\n",
    "\n",
    "$$\n",
    "x_{\\text{normalized}} = \\frac{x - x_{\\text{min}}}{x_{\\text{max}} - x_{\\text{min}}}\n",
    "$$\n",
    "\n",
    "This scales all features to a common range (typically $[0, 1]$).\n",
    "\n",
    "| Feature      | Raw Value | Min     | Max     | Normalized Value |\n",
    "|--------------|-----------|---------|---------|------------------|\n",
    "| Price        | 650,000   | 250,000 | 800,000 | 0.72             |\n",
    "| Bedrooms     | 3         | 1       | 5       | 0.50             |\n",
    "| Lot_Size     | 8,000     | 3,000   | 10,000  | 0.714            |\n",
    "\n",
    "‚û°Ô∏è Now, **each feature contributes fairly** to model training or distance comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå Use Case: Housing Data\n",
    "We are normalizing features from a real estate dataset to prepare it for machine learning analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:52:19.398730Z",
     "start_time": "2025-06-18T15:52:18.956193Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-------------+----------------+-----------------+--------------+------------+\n",
      "| House_ID   |   Price |   Area_sqft |   Num_Bedrooms |   Num_Bathrooms |   Year_Built |   Lot_Size |\n",
      "|------------+---------+-------------+----------------+-----------------+--------------+------------|\n",
      "| H100000    |  574507 |        1462 |              3 |               3 |         2002 |       4878 |\n",
      "| H100001    |  479260 |        1727 |              2 |               2 |         1979 |       4943 |\n",
      "| H100002    |  597153 |        1403 |              5 |               2 |         1952 |       5595 |\n",
      "| H100003    |  728454 |        1646 |              5 |               2 |         1992 |       9305 |\n",
      "| H100004    |  464876 |         853 |              1 |               1 |         1956 |       7407 |\n",
      "+------------+---------+-------------+----------------+-----------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# üî¢ Load and display dataset\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "df = pd.read_csv('data/housing_data.csv')\n",
    "print(tabulate(df.head(), headers='keys', tablefmt='psql',showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé Step 1 ‚Äî Implement Min-Max Normalization on the Housing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning the dataset\n",
    "\n",
    "1. We have removed the `House_ID` column as it is not needed for normalization.\n",
    "2. We will fill missing values in numeric columns with the median of each column to ensure that our normalization does not get affected by NaN values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:52:19.595944Z",
     "start_time": "2025-06-18T15:52:19.585850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+----------------+-----------------+--------------+------------+\n",
      "|   Price |   Area_sqft |   Num_Bedrooms |   Num_Bathrooms |   Year_Built |   Lot_Size |\n",
      "|---------+-------------+----------------+-----------------+--------------+------------|\n",
      "|  574507 |        1462 |              3 |               3 |         2002 |       4878 |\n",
      "|  479260 |        1727 |              2 |               2 |         1979 |       4943 |\n",
      "|  597153 |        1403 |              5 |               2 |         1952 |       5595 |\n",
      "|  728454 |        1646 |              5 |               2 |         1992 |       9305 |\n",
      "|  464876 |         853 |              1 |               1 |         1956 |       7407 |\n",
      "+---------+-------------+----------------+-----------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# ‚úçÔ∏è Implement Min-Max Normalization manually here (no sklearn/numpy)\n",
    "# Normalize: Price, Area_sqft, Num_Bedrooms, Num_Bathrooms, Lot_Size\n",
    "\n",
    "\n",
    "# Drop unnecessary columns (House_ID)\n",
    "df = df.drop(columns=['House_ID'])\n",
    "\n",
    "\n",
    "# Clean the dataset by filling missing values with median\n",
    "numeric_columns = ['Price', 'Area_sqft', 'Num_Bedrooms', 'Num_Bathrooms', 'Year_Built', 'Lot_Size']\n",
    "\n",
    "for col in numeric_columns:\n",
    "    median_value = df[col].median()\n",
    "    df[col] = df[col].fillna(median_value)\n",
    "\n",
    "print(tabulate(df.head(), headers='keys', tablefmt='psql',showindex=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of the Code\n",
    "\n",
    "Copy DataFrame: We create a copy of the original DataFrame to avoid modifying it.\n",
    "\n",
    "Normalization Function: The min_max_normalize function:\n",
    "\n",
    "Computes the minimum and maximum values of the column using Python‚Äôs built-in min and max.\n",
    "\n",
    "Checks if max_val == min_val to avoid division by zero (in which case, we return a list of zeros, as all values are identical).\n",
    "\n",
    "Applies the Min-Max formula using a list comprehension.\n",
    "\n",
    "Apply to Each Column: We loop through the columns, apply the normalization, and store the results in new columns (e.g., Price_normalized).\n",
    "\n",
    "Display Results: We show the normalized columns to verify the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-18T15:53:12.591474Z",
     "start_time": "2025-06-18T15:53:12.581868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------------+-----------------+--------------+------------+\n",
      "|    Price |   Area_sqft |   Num_Bedrooms |   Num_Bathrooms |   Year_Built |   Lot_Size |\n",
      "|----------+-------------+----------------+-----------------+--------------+------------|\n",
      "| 0.485226 |    0.315789 |           0.5  |             1   |    0.722222  |   0.320814 |\n",
      "| 0.387827 |    0.394588 |           0.25 |             0.5 |    0.402778  |   0.326191 |\n",
      "| 0.508384 |    0.298246 |           1    |             0.5 |    0.0277778 |   0.380129 |\n",
      "| 0.642651 |    0.370503 |           1    |             0.5 |    0.583333  |   0.687045 |\n",
      "| 0.373119 |    0.134701 |           0    |             0   |    0.0833333 |   0.53003  |\n",
      "+----------+-------------+----------------+-----------------+--------------+------------+\n"
     ]
    }
   ],
   "source": [
    "# Manual Min-Max Normalization\n",
    "\n",
    "# Create a copy of the dataframe to store normalized values\n",
    "df_normalized = df.copy()\n",
    "\n",
    "# Function to perform Min-Max normalization on a single column\n",
    "def min_max_normalize(column):\n",
    "    min_val = min(column)\n",
    "    max_val = max(column)\n",
    "    # Check for division by zero\n",
    "    if max_val == min_val:\n",
    "        return [0] * len(column)  # Return zeros if all values are the same\n",
    "    return [(x - min_val) / (max_val - min_val) for x in column]\n",
    "\n",
    "# Apply normalization to each column\n",
    "for col in numeric_columns:\n",
    "    df_normalized[f'{col}'] = min_max_normalize(df[col])\n",
    "\n",
    "# # Display the first few rows of the normalized dataframe\n",
    "# df_normalized[[f'{col}_normalized' for col in numeric_columns]].head()\n",
    "\n",
    "print(tabulate(df_normalized.head(), headers='keys', tablefmt='psql',showindex=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîé Talking Point 1 ‚Äî [Some insights about the dataset EDA]\n",
    " \n",
    "We noticed in our repo the dataset did not contain any missing values, so we did not need to fill any NaN values. However we noticed the other team here filled the missing values with the median of each column. Also in order to understand the dataset better, it usually is good practice to add some visualizations and Exploaratory Data Analysis (EDA).\n",
    " \n",
    "Reviwed by:\n",
    "- Eris Leksi\n",
    "- Erica Holden\n",
    "- Reham Abuarqoub\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
